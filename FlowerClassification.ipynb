{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlowerClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOiAtMysrOxs"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.cluster.vq import kmeans,vq\n",
        "from scipy.spatial import distance\n",
        "import random"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsDNaKTzWn9_",
        "outputId": "8e211814-d28d-4bb3-c640-b3187f62f566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install opencv-contrib-python==4.4.0.44"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-contrib-python==4.4.0.44 in /usr/local/lib/python3.6/dist-packages (4.4.0.44)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python==4.4.0.44) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiMJA6F6vnWT"
      },
      "source": [
        "#Replace this code with proper image loading code from gdrive as following\n",
        "#Load images as tuples (img, label)\n",
        "#split into train test pair after shuffling\n",
        "#trainX, trainY, testX, testY is obtained\n",
        "\n",
        "imgs = []\n",
        "\n",
        "for i in range(1,11):\n",
        "  name = 'image_{:04d}.jpg'.format(i)\n",
        "  im = cv2.imread(name,0)\n",
        "  #im = im.astype('uint8')\n",
        "  imgs.append((im, 0))\n",
        "\n",
        "for i in range(89,103):\n",
        "  im = cv2.imread('image_{:04d}.jpg'.format(i),0)\n",
        "  #im = im.astype('uint8')\n",
        "  imgs.append((im, 1))\n",
        "\n",
        "random.shuffle(imgs)\n",
        "train = list(zip(*imgs[:18]))\n",
        "trainX = train[0]\n",
        "trainY = train[1]\n",
        "\n",
        "test = list(zip(*imgs[18:]))\n",
        "testX = test[0]\n",
        "testY = test[1]\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flaUrp5CrdvN"
      },
      "source": [
        "def FeatureExtractor (imgs, extractor):\n",
        "  imgsDescriptors = []\n",
        "  allDescriptors = []\n",
        "  for img in imgs:\n",
        "    keypoint, descriptor = extractor.detectAndCompute(img,None)\n",
        "    imgsDescriptors.append(descriptor)\n",
        "    allDescriptors.extend(descriptor)\n",
        "\n",
        "  return (allDescriptors, imgsDescriptors)\n",
        "\n",
        "def CreateVocabulary (k, descriptors):  \n",
        "  #convert descriptors to float?\n",
        "  vocabulary, distortion = kmeans(descriptors, k, 1)\n",
        "  return vocabulary\n",
        "\n",
        "def ComputeHistogram (imgsDescriptors, vocabulary, k):\n",
        "  histograms = []\n",
        "  for imgDescriptors in imgsDescriptors:\n",
        "    histogram = np.zeros(k)\n",
        "    words, dist = vq(imgDescriptors, vocabulary)\n",
        "    for w in words:\n",
        "      histogram[w] += 1\n",
        "    histograms.append(histogram)\n",
        "  return histograms\n",
        "\n",
        "def Predict (testHistograms, trainHistograms, trainY):\n",
        "  testY = []\n",
        "  for test in testHistograms:\n",
        "    #use eucledian dist for now, change to chi square later\n",
        "    dists = np.square(test[np.newaxis,:] - trainHistograms).sum(axis=1)\n",
        "    mostLike = np.argmin(dists)\n",
        "    testY.append(trainY[mostLike])\n",
        "\n",
        "  return testY\n",
        "\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DOacV5WyFCh"
      },
      "source": [
        "sift = cv2.SIFT_create()\n",
        "(allDescriptors, imgsDescriptorsTrain) = FeatureExtractor(trainX, sift)\n",
        "k = 300\n",
        "vocabulary = CreateVocabulary(k, allDescriptors)\n",
        "histTrain = ComputeHistogram(imgsDescriptorsTrain, vocabulary, k)\n",
        "(ignore, imgsDescriptorsTest) = FeatureExtractor(testX, sift)\n",
        "histTest = ComputeHistogram(imgsDescriptorsTest, vocabulary, k)\n",
        "\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ2jMruP2zyB",
        "outputId": "9157a59d-5ced-4f7e-bee0-ce4755c642fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "testY"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 0, 1, 0, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XaSwuch18lu",
        "outputId": "59139680-0238-49db-e63d-06015fa6c337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Predict(histTest, histTrain, trainY)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 0, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wIhafYbHQED"
      },
      "source": [
        "#Testing on single image\n",
        "\n",
        "im = cv2.imread('image_0001.jpg',0)\n",
        "sift = cv2.SIFT_create()\n",
        "(allDescriptors, imgsDescriptors) = FeatureExtractor([im], sift)\n",
        "k = 20\n",
        "vocabulary = CreateVocabulary(k, allDescriptors)\n",
        "histograms = ComputeHistogram(imgsDescriptors, vocabulary, k)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPDt1brWfrtH"
      },
      "source": [
        "# def FeatureExtractor (classToImgs, extractor):\n",
        "#   #inputs images in the form {classLabel: [images]}\n",
        "#   #outputs ( [all descriptors from all the images], {classLabel:[keypoints x 128 ndarray for each classLabel image]} )\n",
        "#   classToImgDescriptors = {}\n",
        "#   allDescriptors = []\n",
        "\n",
        "#   for classLabel,images in classToImgs.items():\n",
        "#     classToImgDescriptors[classLabel] = []\n",
        "#     for img in images:\n",
        "#       keypoint, descriptor = extractor.detectAndCompute(img,None)\n",
        "#       classToImgDescriptors[classLabel].append(descriptor)\n",
        "#       allDescriptors.extend(descriptor)\n",
        "\n",
        "#   return (allDescriptors, classToImgDescriptors)\n",
        "\n",
        "\n",
        "\n",
        "# def ComputeHistogram (classToImgDescriptors,vocabulary, k):\n",
        "#   classToImageHistograms = {}\n",
        "#   for classes, imgs in classToImgDescriptors.items():\n",
        "#     classToImageHistograms[classes] = []\n",
        "#     for img in imgs:\n",
        "#       histogram = np.zeros(k)\n",
        "#       words, dist = vq(img, vocabulary)\n",
        "#       for w in words:\n",
        "#         histogram[w] += 1\n",
        "#       classToImageHistograms[classes].append(histogram)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}