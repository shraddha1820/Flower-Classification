{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlowerClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOiAtMysrOxs"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.lib import recfunctions as rfn\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import cv2\n",
        "from scipy.cluster.vq import kmeans,vq\n",
        "from scipy.spatial import distance\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler,minmax_scale\n",
        "import pickle\n",
        "import imutils"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "VsDNaKTzWn9_",
        "outputId": "04824b3d-61c2-432a-fc3a-a22fdc94cc17"
      },
      "source": [
        "!pip install opencv-contrib-python==4.4.0.44"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencv-contrib-python==4.4.0.44\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/25/d4aa87bb9279127f1bac881b7524d8bafd25d626aba2c28fa355ed1ada2a/opencv_contrib_python-4.4.0.44-cp36-cp36m-manylinux2014_x86_64.whl (55.7MB)\n",
            "\u001b[K     |████████████████████████████████| 55.7MB 79kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python==4.4.0.44) (1.18.5)\n",
            "Installing collected packages: opencv-contrib-python\n",
            "  Found existing installation: opencv-contrib-python 4.1.2.30\n",
            "    Uninstalling opencv-contrib-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-contrib-python-4.1.2.30\n",
            "Successfully installed opencv-contrib-python-4.4.0.44\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq6E9kb6UJih"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSCnGcvxLu5X",
        "outputId": "caa6ca89-dcc9-442e-af6f-97c26a5ab6d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OH_YD-YM9OH"
      },
      "source": [
        "import glob\n",
        "mypath = \"/content/drive/My Drive/DIP/DIP_flowers\"\n",
        "# l = glob.glob(\"/content/drive/My Drive/DIP/DIP_flowers/image_*\")\n",
        "# random.shuffle(l)\n",
        "# random.shuffle(l)\n",
        "# random.shuffle(l)\n",
        "# ll = [pd.read_csv(path) for path in l]\n",
        "from os import walk\n",
        "f = []\n",
        "for (dirpath, dirnames, filenames) in walk(mypath):\n",
        "    f.extend(filenames)\n",
        "    break\n",
        "for i in range(len(f)):\n",
        "  f[i] = mypath + \"/\" + f[i]\n",
        "df = pd.DataFrame(f,columns=['img'])\n",
        "# f = open(\"/content/drive/My Drive/DIP/DIP_flowers/txt/files.txt\")\n",
        "# data = f.read().splitlines()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjzbyLat6fQ6"
      },
      "source": [
        "\n",
        "df['label']=''\n",
        "df['label'].iloc[0:80]=\"Daffodil\"\n",
        "df['label'].iloc[80:160]=\"Snowdrop\"\n",
        "df['label'].iloc[160:240]=\"LillyValley\"\n",
        "df['label'].iloc[240:320]=\"Bluebell\"\n",
        "df['label'].iloc[320:400]=\"Crocus\"\n",
        "df['label'].iloc[400:480]=\"Iris\"\n",
        "df['label'].iloc[480:560]=\"Tigerlily\"\n",
        "df['label'].iloc[560:640]=\"Tulip\"\n",
        "df['label'].iloc[640:720]=\"Fritillary\"\n",
        "df['label'].iloc[720:800]=\"Sunflower\"\n",
        "df['label'].iloc[800:880]=\"Daisy\"\n",
        "df['label'].iloc[880:1040]=\"Dandelion\"\n",
        "df['label'].iloc[1040:1120]=\"Cowslip\"\n",
        "df['label'].iloc[1120:1200]=\"Buttercup\"\n",
        "df['label'].iloc[1200:1280]=\"Windflower\"\n",
        "df['label'].iloc[1280:1360]=\"Pansy\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "G1gFF6o16fLU",
        "outputId": "806eb523-56c9-410e-a661-ed14be505bdc"
      },
      "source": [
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Daffodil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Daffodil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Daffodil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Daffodil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Daffodil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1355</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Pansy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1356</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Pansy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1357</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Pansy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1358</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Pansy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359</th>\n",
              "      <td>/content/drive/My Drive/DIP/DIP_flowers/image_...</td>\n",
              "      <td>Pansy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1360 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    img     label\n",
              "0     /content/drive/My Drive/DIP/DIP_flowers/image_...  Daffodil\n",
              "1     /content/drive/My Drive/DIP/DIP_flowers/image_...  Daffodil\n",
              "2     /content/drive/My Drive/DIP/DIP_flowers/image_...  Daffodil\n",
              "3     /content/drive/My Drive/DIP/DIP_flowers/image_...  Daffodil\n",
              "4     /content/drive/My Drive/DIP/DIP_flowers/image_...  Daffodil\n",
              "...                                                 ...       ...\n",
              "1355  /content/drive/My Drive/DIP/DIP_flowers/image_...     Pansy\n",
              "1356  /content/drive/My Drive/DIP/DIP_flowers/image_...     Pansy\n",
              "1357  /content/drive/My Drive/DIP/DIP_flowers/image_...     Pansy\n",
              "1358  /content/drive/My Drive/DIP/DIP_flowers/image_...     Pansy\n",
              "1359  /content/drive/My Drive/DIP/DIP_flowers/image_...     Pansy\n",
              "\n",
              "[1360 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T1mmxjiEpA4",
        "outputId": "9b644954-6292-46e3-caf1-e7c1d8cf6d42"
      },
      "source": [
        "\n",
        "df['label'].value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dandelion      160\n",
              "Tulip           80\n",
              "Tigerlily       80\n",
              "Buttercup       80\n",
              "Snowdrop        80\n",
              "Crocus          80\n",
              "Sunflower       80\n",
              "Pansy           80\n",
              "Windflower      80\n",
              "Daffodil        80\n",
              "LillyValley     80\n",
              "Bluebell        80\n",
              "Cowslip         80\n",
              "Iris            80\n",
              "Daisy           80\n",
              "Fritillary      80\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhiuvfxJIuHJ"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, df['label'], test_size=0.2, random_state=0, stratify=df['label'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK3q6FV0J-XB",
        "outputId": "9668e4c9-71e6-47e8-ff9e-5cec47c7f407"
      },
      "source": [
        "X_train.shape,X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1088, 2), (272, 2), (1088,), (272,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRN652rTKKeV",
        "outputId": "c0acc046-8d61-48f5-aa4a-3d32b8f9cb81"
      },
      "source": [
        "\n",
        "X_train['label'].value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dandelion      128\n",
              "Tulip           64\n",
              "Tigerlily       64\n",
              "Buttercup       64\n",
              "Windflower      64\n",
              "Daffodil        64\n",
              "LillyValley     64\n",
              "Bluebell        64\n",
              "Snowdrop        64\n",
              "Cowslip         64\n",
              "Iris            64\n",
              "Crocus          64\n",
              "Sunflower       64\n",
              "Daisy           64\n",
              "Pansy           64\n",
              "Fritillary      64\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0fIyLvdKYa7",
        "outputId": "56ba3de5-2be6-44d1-aea2-6476e7d2e3f0"
      },
      "source": [
        "X_test['label'].value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dandelion      32\n",
              "Iris           16\n",
              "Bluebell       16\n",
              "Buttercup      16\n",
              "Windflower     16\n",
              "Snowdrop       16\n",
              "Cowslip        16\n",
              "Pansy          16\n",
              "Daffodil       16\n",
              "Crocus         16\n",
              "LillyValley    16\n",
              "Tulip          16\n",
              "Fritillary     16\n",
              "Sunflower      16\n",
              "Tigerlily      16\n",
              "Daisy          16\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBydcLbUofjE"
      },
      "source": [
        "\n",
        "\n",
        "file = open('/content/drive/MyDrive/DIP/DIP_flowers/data/img.pkl', 'rb')\n",
        "# dump information to that file\n",
        "data_gray=pickle.load(file)\n",
        "# close the file\n",
        "file.close()\n",
        "X_train_img_arr_gray,X_test_img_arr_gray = train_test_split(data_gray, test_size=0.2, random_state=0, stratify=df['label'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd2PJyjgslPr"
      },
      "source": [
        "file = open('/content/drive/MyDrive/DIP/DIP_flowers/data/all_img_color.pkl', 'rb')\n",
        "# dump information to that file\n",
        "data=pickle.load(file)\n",
        "# close the file\n",
        "file.close()\n",
        "\n",
        "X_train_img_arr,X_test_img_arr = train_test_split(data, test_size=0.2, random_state=0, stratify=df['label'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBXXSIUus03q",
        "outputId": "97d704a8-8720-441b-99a3-3ebcc3bc17fb"
      },
      "source": [
        "print(len(X_train_img_arr_gray))\n",
        "print(len(X_test_img_arr_gray))\n",
        "print(len(X_train_img_arr))\n",
        "print(len(X_test_img_arr))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1088\n",
            "272\n",
            "1088\n",
            "272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAq4bHQL-EIw"
      },
      "source": [
        "# X_train_img_arr=[]\n",
        "# for i in X_train['img']: \n",
        "#   img = cv2.imread(i)\n",
        "#   # print(img)\n",
        "#   # print(img.shape)\n",
        "#   X_train_img_arr.append(img)\n",
        "\n",
        "# X_train_img_arr_gray=[]\n",
        "# for i in X_train_img_arr: \n",
        "#   g=cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
        "#   X_train_img_arr_gray.append(g)\n",
        "\n",
        "# X_test_img_arr=[]\n",
        "# for i in X_test['img']: \n",
        "#   img = cv2.imread(i)\n",
        "#   X_train_img_arr.append(img)\n",
        "\n",
        "# X_test_img_arr_gray=[]\n",
        "# for i in X_test_img_arr:\n",
        "#   g=cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
        "#   X_test_img_arr_gray.append(g)\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiMJA6F6vnWT"
      },
      "source": [
        "#Replace this code with proper image loading code from gdrive as following\n",
        "#Load images as tuples (img, label)\n",
        "#split into train test pair after shuffling\n",
        "#trainX, trainY, testX, testY is obtained\n",
        "\n",
        "#_________________________________________________#\n",
        "\n",
        "# imgs = []\n",
        "# for i in range(1,11):\n",
        "#   name = 'image_{:04d}.jpg'.format(i)\n",
        "#   im = cv2.imread(name,0)\n",
        "#   #im = im.astype('uint8')\n",
        "#   imgs.append((im, 0))\n",
        "\n",
        "# for i in range(89,103):\n",
        "#   im = cv2.imread('image_{:04d}.jpg'.format(i),0)\n",
        "#   #im = im.astype('uint8')\n",
        "#   imgs.append((im, 1))\n",
        "\n",
        "# random.shuffle(imgs)\n",
        "# train = list(zip(*imgs[:18]))\n",
        "# trainX = train[0]\n",
        "# trainY = train[1]\n",
        "\n",
        "# test = list(zip(*imgs[18:]))\n",
        "# testX = test[0]\n",
        "# testY = test[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00s_3hizzBAU"
      },
      "source": [
        "##Shape based Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flaUrp5CrdvN"
      },
      "source": [
        "def FeatureExtractor (imgs, extractor):\n",
        "  imgsDescriptors = []\n",
        "  allDescriptors = []\n",
        "  for img in imgs:\n",
        "    keypoint, descriptor = extractor.detectAndCompute(img,None)\n",
        "    # print(descriptor)\n",
        "    imgsDescriptors.append(descriptor)\n",
        "    allDescriptors.extend(descriptor)\n",
        "\n",
        "  return (allDescriptors, imgsDescriptors)\n",
        "\n",
        "\n",
        "\n",
        "def normalize(hist):\n",
        "  csum = np.sum(hist, axis = 1).reshape(-1,1)\n",
        "  # print(csum)\n",
        "  hist = hist / csum\n",
        "  # print (hist.shape, hist[0].shape)\n",
        "  # print(np.sum(hist, axis = 1))\n",
        "  return list(hist)\n",
        "\n",
        "# def load_img(xtrain):\n",
        "#   if(len(images)!=0)\n",
        "#     images=color_read(xtrain)\n",
        "#   img_arr=gray_read(images)\n",
        "#   # img_arr_train=[]\n",
        "#   # for i in xtrain['img'][:20]: \n",
        "#   #     path = i\n",
        "#   #     img = cv2.imread(path,0)\n",
        "#   #     # print(img)\n",
        "#   #     # print(img.shape)\n",
        "#   #     img_arr_train.append(img)\n",
        "#   return img_arr\n",
        "\n",
        "def CreateVocabulary (k, descriptors):  \n",
        "  #convert descriptors to float?\n",
        "  vocabulary, distortion = kmeans(descriptors, k, 1)\n",
        "  return vocabulary\n",
        "\n",
        "def ComputeHistogram (imgsDescriptors, vocabulary, k):\n",
        "  histograms = []\n",
        "  for imgDescriptors in imgsDescriptors:\n",
        "    histogram = np.zeros(k)\n",
        "    words, dist = vq(imgDescriptors, vocabulary)\n",
        "    for w in words:\n",
        "      histogram[w] += 1\n",
        "    # histogram = histogram.reshape(-1,1)\n",
        "    histograms.append(histogram)\n",
        " \n",
        "  return histograms\n",
        "\n",
        "def Predict (testHistograms, trainHistograms, trainY):\n",
        "  testY = []\n",
        "  for test in testHistograms:\n",
        "    #use eucledian dist for now, change to chi square later\n",
        "    dists = np.square(test[np.newaxis,:] - trainHistograms).sum(axis=1)\n",
        "    mostLike = np.argmin(dists)\n",
        "    testY.append(trainY.iloc[mostLike])\n",
        "  return testY"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DOacV5WyFCh"
      },
      "source": [
        "sift = cv2.SIFT_create()\n",
        "\n",
        "# print(images)\n",
        "(allDescriptors, imgsDescriptorsTrain) = FeatureExtractor(X_train_img_arr_gray, sift)\n",
        "\n",
        "\n",
        "# -----------------------Pickle code--------------------------\n",
        "# open a file, where you ant to store the data\n",
        "file = open('Shape_allDescriptors.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(allDescriptors, file)\n",
        "# close the file\n",
        "file.close()\n",
        "\n",
        "file = open('Shape_imgsDescriptionTrain.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(imgsDescriptorsTrain, file)\n",
        "# close the file\n",
        "file.close()\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "k = 300\n",
        "vocabulary = CreateVocabulary(k, allDescriptors)\n",
        "histTrain = ComputeHistogram(imgsDescriptorsTrain, vocabulary, k)\n",
        "histTrain=normalize(np.array(histTrain))\n",
        "\n",
        "\n",
        "(ignore, imgsDescriptorsTest) = FeatureExtractor(X_test_img_arr_gray, sift)\n",
        "histTest = ComputeHistogram(imgsDescriptorsTest, vocabulary, k)\n",
        "histTest=normalize(np.array(histTest))\n",
        "\n",
        "\n",
        "# -----------------------Pickle code--------------------------\n",
        "# open a file, where you ant to store the data\n",
        "file = open('Shape_hist_train.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(histTrain, file)\n",
        "# close the file\n",
        "file.close()\n",
        "\n",
        "file = open('Shape_hist_test.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(histTest, file)\n",
        "# close the file\n",
        "file.close()\n",
        "# ---------------------------------------------------------------"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XaSwuch18lu"
      },
      "source": [
        "pred_y=Predict(histTest, histTrain, y_train)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMEf84EQ2R0b",
        "outputId": "cbe047ee-0fb4-4e43-d480-49e195277ca0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(y_test, pred_y))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.45588235294117646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wIhafYbHQED"
      },
      "source": [
        "#Testing on single image\n",
        "# im = cv2.imread('image_0001.jpg',0)\n",
        "# sift = cv2.SIFT_create()\n",
        "# (allDescriptors, imgsDescriptors) = FeatureExtractor([im], sift)\n",
        "# k = 20\n",
        "# vocabulary = CreateVocabulary(k, allDescriptors)\n",
        "# histograms = ComputeHistogram(imgsDescriptors, vocabulary, k)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPDt1brWfrtH"
      },
      "source": [
        "# def FeatureExtractor (classToImgs, extractor):\n",
        "#   #inputs images in the form {classLabel: [images]}\n",
        "#   #outputs ( [all descriptors from all the images], {classLabel:[keypoints x 128 ndarray for each classLabel image]} )\n",
        "#   classToImgDescriptors = {}\n",
        "#   allDescriptors = []\n",
        "\n",
        "#   for classLabel,images in classToImgs.items():\n",
        "#     classToImgDescriptors[classLabel] = []\n",
        "#     for img in images:\n",
        "#       keypoint, descriptor = extractor.detectAndCompute(img,None)\n",
        "#       classToImgDescriptors[classLabel].append(descriptor)\n",
        "#       allDescriptors.extend(descriptor)\n",
        "\n",
        "#   return (allDescriptors, classToImgDescriptors)\n",
        "\n",
        "\n",
        "\n",
        "# def ComputeHistogram (classToImgDescriptors,vocabulary, k):\n",
        "#   classToImageHistograms = {}\n",
        "#   for classes, imgs in classToImgDescriptors.items():\n",
        "#     classToImageHistograms[classes] = []\n",
        "#     for img in imgs:\n",
        "#       histogram = np.zeros(k)\n",
        "#       words, dist = vq(img, vocabulary)\n",
        "#       for w in words:\n",
        "#         histogram[w] += 1\n",
        "#       classToImageHistograms[classes].append(histogram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU_r8FpKzIme"
      },
      "source": [
        "##Texture based Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGeGmDerA2YS"
      },
      "source": [
        "### Using LBP(Local Binary Patterns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgMexnYazYyW"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.feature import local_binary_pattern"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3loYQ5TXzY4Q"
      },
      "source": [
        "# def load_image():\n",
        "#     strr = \"Datasets/*.jpg\"\n",
        "#     for file in glob.glob(strr):\n",
        "#         img=np.asarray(plt.imread(file))\n",
        "#         arr.append(img)\n",
        "#     return arr\n",
        "\n",
        "def preprocessing(df):\n",
        "    arr_prep=[]\n",
        "    for i in range(len(df)):\n",
        "        img=cv2.cvtColor(df['img'][i],0)\n",
        "        arr_prep.append(img)\n",
        "    return arr_prep\n",
        "\n",
        "def extractLBP(img):\n",
        "    lbp = local_binary_pattern(img, 24,3, method=\"uniform\")\n",
        "    (hist, _) = np.histogram(lbp.ravel(),bins=np.arange(0, 27),range=(0, 26))                   \n",
        "    hist = hist.astype(\"float\")                         \n",
        "    hist /= (hist.sum() + (1e-7))\n",
        "    return lbp,hist    \n",
        "\n",
        "def featureExtraction(arr):\n",
        "    arr_feature=[]\n",
        "    vector_feature=[]\n",
        "    for i in range(len(arr)):\n",
        "        lb,vektor = extractLBP(arr[i])\n",
        "        # print(lb.shape)\n",
        "        arr_feature.append(lb)\n",
        "        vector_feature.append(vektor)\n",
        "    return arr_feature, vector_feature\n",
        "\n",
        "\n",
        "def normalize(hist):\n",
        "  csum = np.sum(hist, axis = 1).reshape(-1,1)\n",
        "  # print(csum)\n",
        "  hist = hist / csum\n",
        "  # print (hist.shape, hist[0].shape)\n",
        "  # print(np.sum(hist, axis = 1))\n",
        "  return list(hist)\n",
        "\n",
        "def Predict (testHistograms, trainHistograms, trainY):\n",
        "  testY = []\n",
        "  for test in testHistograms:\n",
        "    #use eucledian dist for now, change to chi squaallDescriptors, imgsDescriptorsre later\n",
        "    dists = np.square(test[np.newaxis,:] - trainHistograms).sum(axis=1)       #using normal distance\n",
        "    # dist = cv2.compareHist(test,train, cv2.HISTCMP_CHISQR)\n",
        "    mostLike = np.argmin(dists)\n",
        "    # print(mostLike)\n",
        "    # print(\"---\",trainY.iloc[mostLike])\n",
        "    testY.append(trainY.iloc[mostLike])\n",
        "  return testY\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2FRDYKkzYSV"
      },
      "source": [
        "dataExtracted,vector_hist_train = featureExtraction(X_train_img_arr_gray)\n",
        "vector_hist_train=normalize(vector_hist_train)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTlHanU7zYXX"
      },
      "source": [
        "# arr_test= []\n",
        "# arr_test=load_img(X_test)    #shape feature's load function used \n",
        "dataExtracted,vector_hist_test = featureExtraction(X_test_img_arr_gray)\n",
        "vector_hist_test=normalize(vector_hist_test)\n",
        "\n",
        "\n",
        "# # -----------------------Pickle code--------------------------\n",
        "# # open a file, where you ant to store the data\n",
        "# file = open('Texture_hist_train.pkl', 'wb')\n",
        "# # dump information to that file\n",
        "# pickle.dump(vector_hist_train, file)\n",
        "# # close the file\n",
        "# file.close()\n",
        "\n",
        "# file = open('Texture_hist_test.pkl', 'wb')\n",
        "# # dump information to that file\n",
        "# pickle.dump(vector_hist_test, file)\n",
        "# # close the file\n",
        "# file.close()\n",
        "# # ---------------------------------------------------------------\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmOpUyrizXxH"
      },
      "source": [
        "# print('Extraction Result')\n",
        "# fig = plt.figure(figsize=(10,10))\n",
        "# ax1 = fig.add_subplot(4,2,1)\n",
        "# ax1.set_title('Before')\n",
        "# ax1.set_axis_off()\n",
        "# ax1.imshow(arr_prep[0])\n",
        "\n",
        "# ax2 = fig.add_subplot(4,2,2)\n",
        "# ax2.set_title('After')\n",
        "# ax2.set_axis_off()\n",
        "# ax2.imshow(dataExtracted[2],cmap=plt.cm.gray)\n",
        "# plt.show()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgFl33DB_dTp"
      },
      "source": [
        "# print(\"Vector of Image 1 :\",vector[0])\n",
        "# print()\n",
        "# print(\"Vector of Image 2 :\",vector[1])\n",
        "# print()\n",
        "# print(\"Vector of Image 3 :\",vector[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbUgLk9Yzp8T"
      },
      "source": [
        "pred_y=Predict(vector_hist_test,vector_hist_train,y_train)\n",
        "# print(pred_y)\n",
        "# print(y_test[0:20])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcu4hmqg0y4t",
        "outputId": "1e03914d-4572-4b39-d048-38e90a434478"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(y_test, pred_y))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.14338235294117646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pEAVL95_Mph"
      },
      "source": [
        "## Color Based Extraction:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHGkbV0j_LlC"
      },
      "source": [
        "def FeatureExtractor (imgs):\n",
        "  imgsDescriptors = []\n",
        "  allDescriptors = []\n",
        "  for img in imgs:\n",
        "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV).astype('double')\n",
        "    # print(list(hsv_img[:,:,0]))\n",
        "    # print(hsv_img[:,:,0].flatten().tolist())\n",
        "    # print(hsv_img[:,:].flatten().tolist())\n",
        "    x =rfn.unstructured_to_structured(hsv_img)\n",
        "    # print(x.flatten().tolist()[0:5])\n",
        "    x=x.flatten().tolist()\n",
        "    imgsDescriptors.append(x)\n",
        "    allDescriptors.extend(x)\n",
        "  return (allDescriptors, imgsDescriptors)\n",
        "\n",
        "def Resize(imgs,dim=50):\n",
        "  new_imgs=[]\n",
        "  for im in imgs:\n",
        "    if im.shape[0] < im.shape[1]:\n",
        "      resized = imutils.resize(im, height=dim) \n",
        "    else:\n",
        "      resized = imutils.resize(im, width=dim)\n",
        "    # print(resized.shape)\n",
        "    new_imgs.append(resized)\n",
        "  return new_imgs\n",
        "\n",
        "\n",
        "\n",
        "def CreateVocabulary (k, descriptors):  \n",
        "  #convert descriptors to float?\n",
        "  # print(descriptors)\n",
        "  vocabulary=MiniBatchKMeans(n_clusters=k, batch_size = 1000000).fit(descriptors)\n",
        "  # vocabulary, distortion = kmeans(descriptors, k, 1, check_finite=False)\n",
        "  # vocabulary, distortion = kmeans(descriptors, k, 1)\n",
        "  return vocabulary.cluster_centers_\n",
        "  # return vocabulary\n",
        "\n",
        "def ComputeHistogram (imgsDescriptors, vocabulary, k):\n",
        "  histograms = []\n",
        "  for imgDescriptors in imgsDescriptors:\n",
        "    histogram = np.zeros(k)\n",
        "    words, dist = vq(imgDescriptors, vocabulary)\n",
        "    for w in words:\n",
        "      histogram[w] += 1\n",
        "    # histogram = histogram.reshape(-1)\n",
        "    histograms.append(histogram)\n",
        "  return histograms\n",
        "\n",
        "def Predict (testHistograms, trainHistograms, trainY):\n",
        "  testY = []\n",
        "  for test in testHistograms:\n",
        "    #use eucledian dist for now, change to chi squaallDescriptors, imgsDescriptorsre later\n",
        "    dists = np.square(test[np.newaxis,:] - trainHistograms).sum(axis=1)       #using normal distance\n",
        "    # dist = cv2.compareHist(test,train, cv2.HISTCMP_CHISQR)\n",
        "    mostLike = np.argmin(dists)\n",
        "    # print(mostLike)\n",
        "    # print(\"---\",trainY.iloc[mostLike])\n",
        "    testY.append(trainY.iloc[mostLike])\n",
        "  return testY\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37FpzK3Thrkf"
      },
      "source": [
        "from skimage.color import rgb2hsv\n",
        "\n",
        "# X_train_img_arr=[]\n",
        "# for i in X_train['img'][:1]: \n",
        "#   img = cv2.imread(i)\n",
        "#   # print(img)\n",
        "#   # print(img.shape)\n",
        "#   X_train_img_arr.append(img)\n",
        "\n",
        "X_train_img_arr=Resize(X_train_img_arr)\n",
        "\n",
        "allDescriptors_train, imgsDescriptors_train=FeatureExtractor(X_train_img_arr)\n",
        "\n",
        "\n",
        "# # -----------------------Pickle code--------------------------\n",
        "# # open a file, where you ant to store the data\n",
        "# file = open('Color_All_Descriptors.pkl', 'wb')\n",
        "# # dump information to that file\n",
        "# pickle.dump(allDescriptors_train, file)\n",
        "# # close the file\n",
        "# file.close()\n",
        "\n",
        "# file = open('Color_Image_Descriptors.pkl', 'wb')\n",
        "# # dump information to that file\n",
        "# pickle.dump(imgsDescriptors_train, file)\n",
        "# # close the file\n",
        "# file.close()\n",
        "# # ---------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDzbl7Cqkvpf",
        "outputId": "9d99a736-b192-47b0-b608-d4a717f2e153"
      },
      "source": [
        "\n",
        "k = 300\n",
        "print(\"Features Extracted train\")\n",
        "vocabulary = CreateVocabulary(k, allDescriptors_train)\n",
        "print(\"Vocabulary Created train\")\n",
        "hist_train = ComputeHistogram(imgsDescriptors_train, vocabulary, k)\n",
        "hist_train=normalize(np.array(hist_train))\n",
        "# print(\"Histogram ban gaya train\")\n",
        "# # print(hist_train)\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features Extracted train\n",
            "Vocabulary Created train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Anc9aso_m-"
      },
      "source": [
        "# -----------------------Pickle code--------------------------\n",
        "# open a file, where you ant to store the data\n",
        "file = open('Color_AllDescriptorsTrain', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(allDescriptors_train, file)\n",
        "# close the file\n",
        "file.close()\n",
        "\n",
        "file = open('Color_ImgDescriptorsTrain', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(imgsDescriptors_train, file)\n",
        "# close the file\n",
        "file.close()\n",
        "# file = open('important', 'rb') # dump information to that file \n",
        "# data = pickle.load(file)       # close the file \n",
        "# file.close()\n",
        "# ---------------------------------------------------------------\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R-XnzYikv1l"
      },
      "source": [
        "\n",
        "X_test_img_arr=Resize(X_test_img_arr)\n",
        "\n",
        "allDescriptors_test, imgsDescriptors_test=FeatureExtractor(X_test_img_arr)\n",
        "hist_test = ComputeHistogram(imgsDescriptors_test, vocabulary, k)\n",
        "hist_test=normalize(np.array(hist_test))\n",
        "# for i in hist_train:\n",
        "#   print(i)\n",
        "# print(type(hist_train))\n",
        "\n",
        "\n",
        "\n",
        "color_hist_train=hist_train\n",
        "color_hist_test=hist_test\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHBRXM2KpGv9"
      },
      "source": [
        "# -----------------------Pickle code--------------------------\n",
        "\n",
        "file = open('Color_AllDescriptorsTest.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(allDescriptors_test, file)\n",
        "# close the file\n",
        "file.close()\n",
        "\n",
        "file = open('Color_ImgDescriptorsTest.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(imgsDescriptors_test, file)\n",
        "# close the file\n",
        "file.close()\n",
        "\n",
        "\n",
        "# open a file, where you ant to store the data\n",
        "file = open('Color_Histogram_train.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(hist_train, file)\n",
        "# close the file\n",
        "file.close()\n",
        "\n",
        "file = open('Color_Histogram_test.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(hist_test, file)\n",
        "# close the file\n",
        "file.close()\n",
        "# file = open('Color_Histogram_test.pkl', 'wb')\n",
        "# # dump information to that file\n",
        "# pickle.dump(hist_test, file)\n",
        "# # close the file\n",
        "# file.close()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L58LgnBayGE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ed234d-02ce-4218-ac17-0a20e4b5f88e"
      },
      "source": [
        "pred_y=Predict(hist_test,hist_train,y_train)\n",
        "# print(pred_y)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(y_test, pred_y))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3161764705882353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay_ar3C7fq2y"
      },
      "source": [
        "def Predict (testHistogramsS, trainHistogramsS, testHistogramsT, trainHistogramsT,testHistogramsC, trainHistogramsC, trainY, a1=1, a2=1, a3=0.4):\n",
        "  testY = []\n",
        "  for testS, testT, testC in zip(testHistogramsS,testHistogramsT, testHistogramsC):\n",
        "    #use eucledian dist for now, change to chi squaallDescriptors, imgsDescriptorsre later\n",
        "    distsS = np.square(testS[np.newaxis,:] - trainHistogramsS).sum(axis=1)  \n",
        "    distsT = np.square(testT[np.newaxis,:] - trainHistogramsT).sum(axis=1)  \n",
        "    distsC = np.square(testC[np.newaxis,:] - trainHistogramsC).sum(axis=1) \n",
        "    print()\n",
        "    dists = a1 *  distsS + a2* distsT + a3 * distsC\n",
        "    mostLike = np.argmin(dists)\n",
        "    # print(mostLike)\n",
        "    # print(\"---\",trainY.iloc[mostLike])\n",
        "    testY.append(trainY.iloc[mostLike])\n",
        "  return testY\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRbCO_v-fq92"
      },
      "source": [
        "file = open('/content/drive/MyDrive/DIP/DIP_flowers/data/Shape_hist_train.pkl', 'rb') # dump information to that file \n",
        "shape_hist_train= pickle.load(file)       # close the file \n",
        "file.close()\n",
        "\n",
        "file = open('/content/drive/MyDrive/DIP/DIP_flowers/data/Shape_hist_test.pkl', 'rb') # dump information to that file \n",
        "shape_hist_test= pickle.load(file)       # close the file \n",
        "file.close()\n",
        "\n",
        "\n",
        "file = open('/content/drive/MyDrive/DIP/DIP_flowers/data/Texture_hist_test.pkl', 'rb') # dump information to that file \n",
        "texture_hist_test= pickle.load(file)       # close the file \n",
        "file.close()\n",
        "\n",
        "\n",
        "file = open('/content/drive/MyDrive/DIP/DIP_flowers/data/Texture_hist_train.pkl', 'rb') # dump information to that file \n",
        "texture_hist_train= pickle.load(file)       # close the file \n",
        "file.close()\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kOd5nYBs4UZ",
        "outputId": "3cd11f63-0d05-4f6e-86e3-0a0f8c05a68d"
      },
      "source": [
        "pred_combi=Predict(shape_hist_test[-272:],shape_hist_train,texture_hist_test[-272:],texture_hist_test,color_hist_test,color_hist_train[0:952],y_train)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKlxyYTnuj_Z"
      },
      "source": [
        "pred_y=Predict(hist_test,hist_train,y_train)\n",
        "# print(pred_y)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(y_test, pred_combi))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}